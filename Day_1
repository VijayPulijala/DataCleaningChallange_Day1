Welcome to day 1 of the 5-Day Data Challenge! Today, we're going to be looking at how to deal with missing values. To get started, click the blue "Fork Notebook" button in the upper, right hand corner. This will create a private copy of this notebook that you can edit and play with. Once you're finished with the exercises, you can choose to make your notebook public to share with others. :)

Your turn! As we work through this notebook, you'll see some notebook cells (a block of either code or text) that has "Your Turn!" written in it. These are exercises for you to do to help cement your understanding of the concepts we're talking about. Once you've written the code to answer a specific question, you can run the code by clicking inside the cell (box with code in it) with the code you want to run and then hit CTRL + ENTER (CMD + ENTER on a Mac). You can also click in a cell and then click on the right "play" arrow to the left of the code. If you want to run all the code in your notebook, you can use the double, "fast forward" arrows at the bottom of the notebook editor.

Here's what we're going to do today:

Take a first look at the data
See how many missing data points we have
Figure out why the data is missing
Drop missing values
Filling in missing values
Let's get started!

Take a first look at the data
The first thing we'll need to do is load in the libraries and datasets we'll be using. For today, I'll be using a dataset of events that occured in American Football games for demonstration, and you'll be using a dataset of building permits issued in San Francisco.

Important! Make sure you run this cell yourself or the rest of your code won't work!

In [1]:
# modules we'll use
import pandas as pd
import numpy as np


# read in all our data
nfl_data = pd.read_csv("../input/nflplaybyplay2009to2016/NFL Play by Play 2009-2017 (v4).csv")
sf_permits = pd.read_csv("../input/building-permit-applications-data/Building_Permits.csv")

# set seed for reproducibility
np.random.seed(0) 
/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2698: DtypeWarning: Columns (25,51) have mixed types. Specify dtype option on import or set low_memory=False.
  interactivity=interactivity, compiler=compiler, result=result)
/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2698: DtypeWarning: Columns (22,32) have mixed types. Specify dtype option on import or set low_memory=False.
  interactivity=interactivity, compiler=compiler, result=result)
The first thing I do when I get a new dataset is take a look at some of it. This lets me see that it all read in correctly and get an idea of what's going on with the data. In this case, I'm looking to see if I see any missing values, which will be reprsented with NaN or None.

In [2]:
# look at a few rows of the nfl_data file. I can see a handful of missing data already!
nfl_data.sample(5)
Out[2]:
Date	GameID	Drive	qtr	down	time	TimeUnder	TimeSecs	PlayTimeDiff	SideofField	...	yacEPA	Home_WP_pre	Away_WP_pre	Home_WP_post	Away_WP_post	Win_Prob	WPA	airWPA	yacWPA	Season
244485	2014-10-26	2014102607	18	3	1.0	00:39	1	939.0	12.0	TB	...	1.240299	0.225647	0.774353	0.245582	0.754418	0.225647	0.019935	-0.018156	0.038091	2014
115340	2011-11-20	2011112000	22	4	1.0	06:47	7	407.0	44.0	OAK	...	NaN	0.056036	0.943964	0.042963	0.957037	0.943964	0.013073	NaN	NaN	2011
68357	2010-11-14	2010111401	8	2	NaN	00:23	1	1823.0	0.0	CLE	...	NaN	0.365307	0.634693	0.384697	0.615303	0.634693	-0.019390	NaN	NaN	2010
368377	2017-09-24	2017092405	24	4	1.0	08:48	9	528.0	8.0	CLE	...	1.075660	0.935995	0.064005	0.921231	0.078769	0.064005	0.014764	0.003866	0.010899	2017
384684	2017-11-05	2017110505	11	2	1.0	09:15	10	2355.0	0.0	DEN	...	NaN	0.928474	0.071526	0.934641	0.065359	0.071526	-0.006166	NaN	NaN	2017
5 rows × 102 columns

Yep, it looks like there's some missing values. What about in the sf_permits dataset?

In [3]:
# your turn! Look at a couple of rows from the sf_permits dataset. Do you notice any missing data?
sf_permits.sample(5)
# your code goes here :)
Out[3]:
Permit Number	Permit Type	Permit Type Definition	Permit Creation Date	Block	Lot	Street Number	Street Number Suffix	Street Name	Street Suffix	...	Existing Construction Type	Existing Construction Type Description	Proposed Construction Type	Proposed Construction Type Description	Site Permit	Supervisor District	Neighborhoods - Analysis Boundaries	Zipcode	Location	Record ID
78651	201503120624	8	otc alterations permit	03/12/2015	4273	029	2986	NaN	26th	St	...	5.0	wood frame (5)	5.0	wood frame (5)	NaN	9.0	Mission	94110.0	(37.74972976527956, -122.40963202424835)	1374057173006
177779	201707071220	8	otc alterations permit	07/07/2017	0453	004A	950	NaN	Bay	St	...	5.0	wood frame (5)	5.0	wood frame (5)	NaN	2.0	Russian Hill	94109.0	(37.80472551510833, -122.42280760820965)	1469546420378
64978	M529127	8	otc alterations permit	10/21/2014	0478	011	1290	NaN	Chestnut	St	...	NaN	NaN	NaN	NaN	NaN	2.0	Russian Hill	94109.0	(37.80244997614665, -122.42443255018165)	1359656465959
165958	201704073507	8	otc alterations permit	04/07/2017	6507	003A	1134	NaN	Noe	St	...	5.0	wood frame (5)	5.0	wood frame (5)	NaN	8.0	Noe Valley	94114.0	(37.750866393629146, -122.43209633372915)	1458922436280
147902	201701237639	8	otc alterations permit	01/23/2017	0623	001	1755	NaN	Van Ness	Av	...	5.0	wood frame (5)	5.0	wood frame (5)	NaN	2.0	Pacific Heights	94109.0	(37.791925784456105, -122.42306858292103)	1450933235988
5 rows × 43 columns

See how many missing data points we have
Ok, now we know that we do have some missing values. Let's see how many we have in each column.

In [4]:
# get the number of missing data points per column
missing_values_count = nfl_data.isnull().sum()

# look at the # of missing points in the first ten columns
missing_values_count[0:10]
Out[4]:
Date                0
GameID              0
Drive               0
qtr                 0
down            61154
time              224
TimeUnder           0
TimeSecs          224
PlayTimeDiff      444
SideofField       528
dtype: int64
That seems like a lot! It might be helpful to see what percentage of the values in our dataset were missing to give us a better sense of the scale of this problem:

In [5]:
# how many total missing values do we have?
total_cells = np.product(nfl_data.shape)
total_missing = missing_values_count.sum()

# percent of data that is missing
(total_missing/total_cells) * 100
Out[5]:
24.87214126835169
Wow, almost a quarter of the cells in this dataset are empty! In the next step, we're going to take a closer look at some of the columns with missing values and try to figure out what might be going on with them.

In [6]:
# your turn! Find out what percent of the sf_permits dataset is missing
missing_values_count_sf = sf_permits.isnull().sum()
total_cells_sf = np.product(sf_permits.shape)
total_missing_sf = missing_values_count_sf.sum()
(total_missing_sf/total_cells_sf) * 100
Out[6]:
26.26002315058403
Figure out why the data is missing
This is the point at which we get into the part of data science that I like to call "data intution", by which I mean "really looking at your data and trying to figure out why it is the way it is and how that will affect your analysis". It can be a frustrating part of data science, especially if you're newer to the field and don't have a lot of experience. For dealing with missing values, you'll need to use your intution to figure out why the value is missing. One of the most important question you can ask yourself to help figure this out is this:

Is this value missing becuase it wasn't recorded or becuase it dosen't exist?

If a value is missing becuase it doens't exist (like the height of the oldest child of someone who doesn't have any children) then it doesn't make sense to try and guess what it might be. These values you probalby do want to keep as NaN. On the other hand, if a value is missing becuase it wasn't recorded, then you can try to guess what it might have been based on the other values in that column and row. (This is called "imputation" and we'll learn how to do it next! :)

Let's work through an example. Looking at the number of missing values in the nfl_data dataframe, I notice that the column TimesSec has a lot of missing values in it:

In [7]:
# look at the # of missing points in the first ten columns
missing_values_count[0:10]
Out[7]:
Date                0
GameID              0
Drive               0
qtr                 0
down            61154
time              224
TimeUnder           0
TimeSecs          224
PlayTimeDiff      444
SideofField       528
dtype: int64
In [8]:
By looking at [the documentation](https://www.kaggle.com/maxhorowitz/nflplaybyplay2009to2016), I can see that this column has information on the number of seconds left in the game when the play was made. This means that these values are probably missing because they were not recorded, rather than because they don't exist. So, it would make sense for us to try and guess what they should be rather than just leaving them as NA's.

On the other hand, there are other fields, like `PenalizedTeam` that also have lot of missing fields. In this case, though, the field is missing because if there was no penalty then it doesn't make sense to say *which* team was penalized. For this column, it would make more sense to either leave it empty or to add a third value like "neither" and use that to replace the NA's.

> **Tip:** This is a great place to read over the dataset documentation if you haven't already! If you're working with a dataset that you've gotten from another person, you can also try reaching out to them to get more information.

If you're doing very careful data analysis, this is the point at which you'd look at each column individually to figure out the best strategy for filling those missing values. For the rest of this notebook, we'll cover some "quick and dirty" techniques that can help you with missing values but will probably also end up removing some useful information or adding some noise to your data.

## Your turn!

* Look at the columns `Street Number Suffix` and `Zipcode` from the `sf_permits` datasets. Both of these contain missing values. Which, if either, of these are missing because they don't exist? Which, if either, are missing because they weren't recorded?


missing_values_count_sf[0:10]

#Most of street number suffix is null 
sf_permits[['Street Number Suffix','Zipcode']].sample(10)

sf_permits_streetnumber_missing = sf_permits['Street Number Suffix'].isnull().sum()
sf_permits_streetzipcode_missing = sf_permits['Zipcode'].isnull().sum()

print("Street Number Suffix is missing for "+str((sf_permits_streetnumber_missing/sf_permits.shape[0])*100))

print("Zipcode is Missing for "+str((sf_permits_streetzipcode_missing/sf_permits.shape[0])*100))
  File "<ipython-input-8-6269001b6e11>", line 1
    By looking at [the documentation](https://www.kaggle.com/maxhorowitz/nflplaybyplay2009to2016), I can see that this column has information on the number of seconds left in the game when the play was made. This means that these values are probably missing because they were not recorded, rather than because they don't exist. So, it would make sense for us to try and guess what they should be rather than just leaving them as NA's.
             ^
SyntaxError: invalid syntax
Code
Drop missing values
If you're in a hurry or don't have a reason to figure out why your values are missing, one option you have is to just remove any rows or columns that contain missing values. (Note: I don't generally recommend this approch for important projects! It's usually worth it to take the time to go through your data and really look at all the columns with missing values one-by-one to really get to know your dataset.)

If you're sure you want to drop rows with missing values, pandas does have a handy function, dropna() to help you do this. Let's try it out on our NFL dataset!

In [9]:
# remove all the rows that contain a missing value
nfl_data.dropna()
Out[9]:
Date	GameID	Drive	qtr	down	time	TimeUnder	TimeSecs	PlayTimeDiff	SideofField	...	yacEPA	Home_WP_pre	Away_WP_pre	Home_WP_post	Away_WP_post	Win_Prob	WPA	airWPA	yacWPA	Season
0 rows × 102 columns

Oh dear, it looks like that's removed all our data! 😱 This is because every row in our dataset had at least one missing value. We might have better luck removing all the columns that have at least one missing value instead.

In [10]:
# remove all columns with at least one missing value
columns_with_na_dropped = nfl_data.dropna(axis=1)
columns_with_na_dropped.head()
Out[10]:
Date	GameID	Drive	qtr	TimeUnder	ydstogo	ydsnet	PlayAttempted	Yards.Gained	sp	...	Timeout_Indicator	Timeout_Team	posteam_timeouts_pre	HomeTimeouts_Remaining_Pre	AwayTimeouts_Remaining_Pre	HomeTimeouts_Remaining_Post	AwayTimeouts_Remaining_Post	ExPoint_Prob	TwoPoint_Prob	Season
0	2009-09-10	2009091000	1	1	15	0	0	1	39	0	...	0	None	3	3	3	3	3	0.0	0.0	2009
1	2009-09-10	2009091000	1	1	15	10	5	1	5	0	...	0	None	3	3	3	3	3	0.0	0.0	2009
2	2009-09-10	2009091000	1	1	15	5	2	1	-3	0	...	0	None	3	3	3	3	3	0.0	0.0	2009
3	2009-09-10	2009091000	1	1	14	8	2	1	0	0	...	0	None	3	3	3	3	3	0.0	0.0	2009
4	2009-09-10	2009091000	1	1	14	8	2	1	0	0	...	0	None	3	3	3	3	3	0.0	0.0	2009
5 rows × 41 columns

In [11]:
# just how much data did we lose?
print("Columns in original dataset: %d \n" % nfl_data.shape[1])
print("Columns with na's dropped: %d" % columns_with_na_dropped.shape[1])
Columns in original dataset: 102 

Columns with na's dropped: 41
We've lost quite a bit of data, but at this point we have successfully removed all the NaN's from our data.

In [12]:
# Your turn! Try removing all the rows from the sf_permits dataset that contain missing values. How many are left?
sf_permits.dropna()
Out[12]:
Permit Number	Permit Type	Permit Type Definition	Permit Creation Date	Block	Lot	Street Number	Street Number Suffix	Street Name	Street Suffix	...	Existing Construction Type	Existing Construction Type Description	Proposed Construction Type	Proposed Construction Type Description	Site Permit	Supervisor District	Neighborhoods - Analysis Boundaries	Zipcode	Location	Record ID
0 rows × 43 columns

In [13]:
# Now try removing all the columns with empty values. Now how much of your data is left?
columns_with_na_dropped_sf = sf_permits.dropna(axis=1)
columns_with_na_dropped_sf.head

print("Columns in original sf_permits dataset: %d \n" % sf_permits.shape[1])
print("Columns with na's sf_permits dropped: %d" % columns_with_na_dropped_sf.shape[1])
Columns in original sf_permits dataset: 43 

Columns with na's sf_permits dropped: 12
Filling in missing values automatically
Another option is to try and fill in the missing values. For this next bit, I'm getting a small sub-section of the NFL data so that it will print well.

In [14]:
# get a small subset of the NFL dataset
subset_nfl_data = nfl_data.loc[:, 'EPA':'Season'].head()
subset_nfl_data
Out[14]:
EPA	airEPA	yacEPA	Home_WP_pre	Away_WP_pre	Home_WP_post	Away_WP_post	Win_Prob	WPA	airWPA	yacWPA	Season
0	2.014474	NaN	NaN	0.485675	0.514325	0.546433	0.453567	0.485675	0.060758	NaN	NaN	2009
1	0.077907	-1.068169	1.146076	0.546433	0.453567	0.551088	0.448912	0.546433	0.004655	-0.032244	0.036899	2009
2	-1.402760	NaN	NaN	0.551088	0.448912	0.510793	0.489207	0.551088	-0.040295	NaN	NaN	2009
3	-1.712583	3.318841	-5.031425	0.510793	0.489207	0.461217	0.538783	0.510793	-0.049576	0.106663	-0.156239	2009
4	2.097796	NaN	NaN	0.461217	0.538783	0.558929	0.441071	0.461217	0.097712	NaN	NaN	2009
We can use the Panda's fillna() function to fill in missing values in a dataframe for us. One option we have is to specify what we want the NaN values to be replaced with. Here, I'm saying that I would like to replace all the NaN values with 0.

In [15]:
# replace all NA's with 0
subset_nfl_data.fillna(0)
Out[15]:
EPA	airEPA	yacEPA	Home_WP_pre	Away_WP_pre	Home_WP_post	Away_WP_post	Win_Prob	WPA	airWPA	yacWPA	Season
0	2.014474	0.000000	0.000000	0.485675	0.514325	0.546433	0.453567	0.485675	0.060758	0.000000	0.000000	2009
1	0.077907	-1.068169	1.146076	0.546433	0.453567	0.551088	0.448912	0.546433	0.004655	-0.032244	0.036899	2009
2	-1.402760	0.000000	0.000000	0.551088	0.448912	0.510793	0.489207	0.551088	-0.040295	0.000000	0.000000	2009
3	-1.712583	3.318841	-5.031425	0.510793	0.489207	0.461217	0.538783	0.510793	-0.049576	0.106663	-0.156239	2009
4	2.097796	0.000000	0.000000	0.461217	0.538783	0.558929	0.441071	0.461217	0.097712	0.000000	0.000000	2009
I could also be a bit more savvy and replace missing values with whatever value comes directly after it in the same column. (This makes a lot of sense for datasets where the observations have some sort of logical order to them.)

In [16]:
# replace all NA's the value that comes directly after it in the same column, 
# then replace all the reamining na's with 0
subset_nfl_data.fillna(method = 'bfill', axis=0).fillna("0")
Out[16]:
EPA	airEPA	yacEPA	Home_WP_pre	Away_WP_pre	Home_WP_post	Away_WP_post	Win_Prob	WPA	airWPA	yacWPA	Season
0	2.014474	-1.06817	1.14608	0.485675	0.514325	0.546433	0.453567	0.485675	0.060758	-0.032244	0.036899	2009
1	0.077907	-1.06817	1.14608	0.546433	0.453567	0.551088	0.448912	0.546433	0.004655	-0.032244	0.036899	2009
2	-1.402760	3.31884	-5.03142	0.551088	0.448912	0.510793	0.489207	0.551088	-0.040295	0.106663	-0.156239	2009
3	-1.712583	3.31884	-5.03142	0.510793	0.489207	0.461217	0.538783	0.510793	-0.049576	0.106663	-0.156239	2009
4	2.097796	0	0	0.461217	0.538783	0.558929	0.441071	0.461217	0.097712	0	0	2009
Filling in missing values is also known as "imputation", and you can find more exercises on it in this lesson, also linked under the "More practice!" section. First, however, why don't you try replacing some of the missing values in the sf_permit dataset?

In [17]:
# Your turn! Try replacing all the NaN's in the sf_permits data with the one that
# comes directly after it and then 

sf_permits.fillna(method = 'bfill', axis=0).fillna("0")

And that's it for today! If you have any questions, be sure to post them in the comments below or on the forums.

Remember that your notebook is private by default, and in order to share it with other people or ask for help with it, you'll need to make it public. First, you'll need to save a version of your notebook that shows your current work by hitting the "Commit & Run" button. (Your work is saved automatically, but versioning your work lets you go back and look at what it was like at the point you saved it. It also let's you share a nice compiled notebook instead of just the raw code.) Then, once your notebook is finished running, you can go to the Settings tab in the panel to the left (you may have to expand it by hitting the [<] button next to the "Commit & Run" button) and setting the "Visibility" dropdown to "Public".
